{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import *#IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomizacja predyktorów\n",
    "d = {'morning' : 0, 'afternoon' : 1, 'evening': 2, 'night': 3}\n",
    "inv_d = dict((v, k) for k, v in d.items())\n",
    "\n",
    "departs = ['Depart_hour1_There', 'Depart_hour2_There', 'Depart_hour1_Back', 'Depart_hour2_Back']\n",
    "arrives = ['Arrival_hour1_There', 'Arrival_hour2_There', 'Arrival_hour1_Back', 'Arrival_hour2_Back']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def groupped_days(days, bin_size):\n",
    "    bin_size = int(bin_size)\n",
    "    low = days // bin_size * bin_size\n",
    "    up = low + bin_size - 1\n",
    "    return(str(low) + '-' + str(up))\n",
    "\n",
    "def exists(obj, datastructure):\n",
    "    return any([obj == elem for elem in datastructure])\n",
    "\n",
    "@udf\n",
    "def random_num():\n",
    "    return random.randint(0, 3)\n",
    "\n",
    "\n",
    "def subtr(depart, arrive):\n",
    "    global d\n",
    "    if (depart != 'none'):\n",
    "        return abs(d[arrive] - d[depart])\n",
    "    else:\n",
    "        return 10\n",
    "    \n",
    "def update_time_depart():\n",
    "    global inv_d\n",
    "    randInt = random.randint(0, 3)\n",
    "    return inv_d[randInt]\n",
    "\n",
    "def update_time_arrive(depart, arrive):\n",
    "    global inv_d\n",
    "    global subtr\n",
    "    randInt = random.randint(0, 3)\n",
    "    sub = subtr(depart, arrive)\n",
    "    return inv_d[(int(randInt) + int(sub)) % 4]\n",
    "\n",
    "def days_update(days):\n",
    "    if days > 90:\n",
    "        return random.randint(0, 10)\n",
    "    else:\n",
    "        return days\n",
    "    \n",
    "def update_price(full_price, days, country_from, country_to, scrap_time, bin_size):\n",
    "    global days_stats\n",
    "    global bin_stats\n",
    "    bin_size = int(bin_size)\n",
    "    records = bin_stats[(bin_stats.Country_from == country_from) & (bin_stats.Country_to == country_to) & \n",
    "                       (bin_stats.Scrap_time == int(scrap_time))]\n",
    "    bucket = (days // bin_size) * bin_size\n",
    "    array = np.array(records['startingBin'].tolist())\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - bucket)).argmin()\n",
    "    bucket = array[idx]\n",
    "    record = records[records.startingBin == bucket]\n",
    "    # Tutaj chodzi o to, że moze wystapic jakas unikalna wartosc i wtedy nie bedzie dla niej wariancji, wtedy\n",
    "    # Ustalam wariancje na 1\n",
    "    if np.isnan(record['stdev'].item()) == False:\n",
    "        new_price = np.random.normal(loc = record['mean'], scale = record['stdev'], size = 1)[0]\n",
    "    else:\n",
    "        new_price = np.random.normal(loc = record['mean'], scale = 1, size = 1)[0]\n",
    "    new_price = new_price + np.random.rand()\n",
    "    return new_price.tolist()\n",
    "\n",
    "@udf\n",
    "def myRandom(x):\n",
    "    return random.randint(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('MyFirstStandaloneApp')\n",
    "conf.set(\"spark.speculation\",\"false\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 5\n",
    "\n",
    "filepath = \"Structured_data2\"\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "flightsRDD = flightsRDD.filter(lambda line : line != header)\n",
    "colnames = header.split(';')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)\n",
    "parts = flightsRDD.map(lambda line: line.split(';'))\n",
    "sqlContext = SQLContext(sc)\n",
    "dt = sqlContext.createDataFrame(parts, schema)\n",
    "to_cast_int = [\"Flight_id\", \"Days\", \"Journey_time\"]\n",
    "to_cast_double = [\"Price1_There\", \"Price1_Back\", \"Price2_There\", \"Price2_Back\", \"Full_Price\"]\n",
    "for field in to_cast_int:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(IntegerType()))\n",
    "for field in to_cast_double:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(DoubleType()))\n",
    "dt.registerTempTable(\"flights\")\n",
    "\n",
    "df = dt\n",
    "\n",
    "weekDay =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d').strftime('%w'))\n",
    "\n",
    "df = dt.withColumn('weekDay', weekDay(dt['Scrap_date']))\n",
    "df = df.withColumn('grouppedDays', groupped_days(df['Days'], lit(bin_size)))\n",
    "fun = udf(lambda x : int(x.split('-')[0]))\n",
    "df = df.withColumn('startingBin', fun(df.grouppedDays))\n",
    "\n",
    "\n",
    "df1 = df.groupby(['Scrap_time', 'Country_from', 'Country_to', 'startingBin']).agg(\n",
    "F.mean(df.Full_Price).alias('mean'),\n",
    "F.stddev(df.Full_Price).alias('stdev'),\n",
    "F.count(df.Full_Price).alias('count'))\n",
    "\n",
    "df2 = df.groupby(['Scrap_time', 'Country_from', 'Country_to', 'Days']).agg(\n",
    "F.mean(df.Full_Price).alias('mean'),\n",
    "F.stddev(df.Full_Price).alias('stdev'),\n",
    "F.count(df.Full_Price).alias('count'))\n",
    "\n",
    "days_stats = df2.toPandas()\n",
    "bin_stats = df1.toPandas()\n",
    "\n",
    "asint = ['Scrap_time', 'startingBin', 'count', 'Days']\n",
    "asfloat = ['mean', 'stdev']\n",
    "day_col = list(days_stats.columns)\n",
    "bin_col = list(bin_stats.columns)\n",
    "\n",
    "for c1, c2 in zip(day_col, bin_col):\n",
    "    if exists(c1 , day_col):\n",
    "        if exists(c1, asint):\n",
    "            days_stats[c1] = days_stats[c1].astype('int')\n",
    "        elif exists(c1, asfloat):\n",
    "            days_stats[c1] = days_stats[c1].astype('float')\n",
    "    if exists(c2 , bin_col):\n",
    "        if exists(c2, asint):\n",
    "            bin_stats[c2] = bin_stats[c2].astype('int')\n",
    "        elif exists(c2, asfloat):\n",
    "            bin_stats[c2] = bin_stats[c2].astype('float')\n",
    "            \n",
    "filepath = \"Reference_file\"\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "flightsRDD = flightsRDD.filter(lambda line : line != header)\n",
    "colnames = header.split(';')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)\n",
    "parts = flightsRDD.map(lambda line: line.split(';'))\n",
    "sqlContext = SQLContext(sc)\n",
    "dt = sqlContext.createDataFrame(parts, schema)\n",
    "to_cast_int = [\"Flight_id\", \"Days\", \"Journey_time\"]\n",
    "to_cast_double = [\"Price1_There\", \"Price1_Back\", \"Price2_There\", \"Price2_Back\", \"Full_Price\"]\n",
    "for field in to_cast_int:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(IntegerType()))\n",
    "for field in to_cast_double:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(DoubleType()))\n",
    "dt.registerTempTable(\"flights\")\n",
    "\n",
    "datafr = dt.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = copy.deepcopy(datafr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafr = copy.deepcopy(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 136397/136397 [14:29<00:00, 156.84it/s]\n",
      " 60%|████████████████████████████████████████████▍                             | 81821/136397 [08:37<06:12, 146.36it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(1, 60):\n",
    "    # zapisuje i wczytuje, zeby odpowiednio inferowal typy\n",
    "    filename = './Generated/' + str(datafr['Scrap_date'][0]) + '.txt'\n",
    "    datafr.to_csv(filename, sep = ';', header = True, index= False)\n",
    "    datafr = pd.read_csv(filename, sep = ';')\n",
    "    \n",
    "    datafr['Scrap_date'] = datafr.apply(lambda row : str(datetime.strptime(row['Scrap_date'], '%Y-%m-%d') - \n",
    "                                                  timedelta(days = 1)).split(' ')[0], axis = 1)\n",
    "    datafr['Days'] = datafr.apply(lambda row: (row[5] + 1) if (row[5] + 1 < 90 ) else random.randint(0, 10) , axis = 1)\n",
    "        \n",
    "    for depart, arrive in zip(departs, arrives):\n",
    "        datafr[depart] = datafr.apply(lambda row : update_time_depart() if row[depart] != 'none' else 'none' , axis = 1)\n",
    "        datafr[arrive] = datafr.apply(lambda row :  update_time_arrive(row[depart], row[arrive]) if row[arrive] != 'none' else\n",
    "                                      'none' , axis = 1)\n",
    "    tqdm.pandas()\n",
    "    datafr['Full_Price'] = datafr.progress_apply(lambda row: update_price(row[-1], row[5], row[2], row[3], row[1], bin_size), axis = 1)\n",
    "    datafr['Full_Price'] = np.round(datafr['Full_Price'], 2)\n",
    "    datafr.to_csv(filename, sep = ';', header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
