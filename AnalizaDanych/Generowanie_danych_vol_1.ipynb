{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm generowania danych:\n",
    "1. Tworzę jeden zbiór ze wszystkich dotychczasowych scrapowań\n",
    "2. Tworzę tabelę statystyk dla każdego kraju z podziałem na grupy dni (typu 0-4, 5-9 itd.), w tym celu wykonuję grupowanie i agregacje danych. Nie biorę w tym punkcie pod uwagę daty scrapowania. Jako wynik tego punktu otrzymyję tabelę, w której zebrane są średnia i wariancja ceny lotów na danej lini kraj-kraj, w zależności od przedziału czasu od odlotu do wylotu z podziałem na czas scrapowania.\n",
    "3. Wracam do pliku z 2015-05-15.txt czyli pierwszego pliku scrapowania i biorę go jako plik referencyjny.\n",
    "4. Przechodzę po każdym wierszu tego pliku i sprawdzam następujące pola:\n",
    "['Country_from', 'Country_to', 'Days', 'Scrap_time']\n",
    "5. Zmieniam datę scrapowania na dzień wcześniej, Days = Days + 1. Jeżeli days > 90 to resetuję rekord do tego samego z wyprzedzeniem losowanym z przedziału (0, 10).\n",
    "6. Randomizuje godziny odlotów, przy czym z zachowaniem odpowiednich długości lotów. Tzn. robię tak: jak miałem lot morning - afternoon to roznica jest 1. Zatem losuje wylot na np. evening to powrot bede mial night\n",
    "7. Na podstawie tabeli ze statystykami generuję cenę z rozkładu normalnego o podanej w tabeli średniej i warinacji. Updatuje rekord o nową cenę do której dodaję jeszcze całkiem losowy szum z rozkładu normalnego. \n",
    "\n",
    "Reszty predyktorów nie zmieniam. Można też robić jakoś tak, że generować te ceny cząstkowe, ale wydaje mi się, że żeby zachować te kolumny to trzeba policzyc taki sredni udzial tych cen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import *#IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "conf = SparkConf().setAppName('MyFirstStandaloneApp')\n",
    "conf.set(\"spark.speculation\",\"false\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "#sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def groupped_days(days, bin_size):\n",
    "    bin_size = int(bin_size)\n",
    "    low = days // bin_size * bin_size\n",
    "    up = low + bin_size - 1\n",
    "    return(str(low) + '-' + str(up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ustalam szerokosc przedzialu po ktorym bede grupowal dni. Jak ustawie na 5, to bede mial grupy 0-4, 5-9, itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Structured_data2\"\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "flightsRDD = flightsRDD.filter(lambda line : line != header)\n",
    "colnames = header.split(';')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)\n",
    "parts = flightsRDD.map(lambda line: line.split(';'))\n",
    "sqlContext = SQLContext(sc)\n",
    "dt = sqlContext.createDataFrame(parts, schema)\n",
    "to_cast_int = [\"Flight_id\", \"Days\", \"Journey_time\"]\n",
    "to_cast_double = [\"Price1_There\", \"Price1_Back\", \"Price2_There\", \"Price2_Back\", \"Full_Price\"]\n",
    "for field in to_cast_int:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(IntegerType()))\n",
    "for field in to_cast_double:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(DoubleType()))\n",
    "dt.registerTempTable(\"flights\")\n",
    "\n",
    "df = dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj tworzę takie pomocnicze kolumny, do dalszej obróbki. Robię też zmienną weekDay, która oznacza numer dnia tygodnia, ale nie korzystam z tego ostatecznie w żaden sposób. (Uwaga. 0 - niedziela, 1-poniedzialek itd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekDay =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d').strftime('%w'))\n",
    "\n",
    "df = dt.withColumn('weekDay', weekDay(dt['Scrap_date']))\n",
    "df = df.withColumn('grouppedDays', groupped_days(df['Days'], lit(bin_size)))\n",
    "fun = udf(lambda x : int(x.split('-')[0]))\n",
    "df = df.withColumn('startingBin', fun(df.grouppedDays))\n",
    "#df = df.withColumn('daysToLeave', df['Days'] + df.['Journey_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj tworzenie tabel statystycznych do generowania danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(['Scrap_time', 'Country_from', 'Country_to', 'startingBin']).agg(\n",
    "F.mean(df.Full_Price).alias('mean'),\n",
    "F.stddev(df.Full_Price).alias('stdev'),\n",
    "F.count(df.Full_Price).alias('count'))\n",
    "\n",
    "df2 = df.groupby(['Scrap_time', 'Country_from', 'Country_to', 'Days']).agg(\n",
    "F.mean(df.Full_Price).alias('mean'),\n",
    "F.stddev(df.Full_Price).alias('stdev'),\n",
    "F.count(df.Full_Price).alias('count'))\n",
    "\n",
    "days_stats = df2.toPandas()\n",
    "bin_stats = df1.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uporządkowanie typów w dataframe'ach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(obj, datastructure):\n",
    "    return any([obj == elem for elem in datastructure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "asint = ['Scrap_time', 'startingBin', 'count', 'Days']\n",
    "asfloat = ['mean', 'stdev']\n",
    "day_col = list(days_stats.columns)\n",
    "bin_col = list(bin_stats.columns)\n",
    "\n",
    "for c1, c2 in zip(day_col, bin_col):\n",
    "    if exists(c1 , day_col):\n",
    "        if exists(c1, asint):\n",
    "            days_stats[c1] = days_stats[c1].astype('int')\n",
    "        elif exists(c1, asfloat):\n",
    "            days_stats[c1] = days_stats[c1].astype('float')\n",
    "    if exists(c2 , bin_col):\n",
    "        if exists(c2, asint):\n",
    "            bin_stats[c2] = bin_stats[c2].astype('int')\n",
    "        elif exists(c2, asfloat):\n",
    "            bin_stats[c2] = bin_stats[c2].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj właściwe generowanie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie pliku referencyjnego, czyli pliku z najstarszego scrapowania: 2019-05-15.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Reference_file\"\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "flightsRDD = flightsRDD.filter(lambda line : line != header)\n",
    "colnames = header.split(';')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)\n",
    "parts = flightsRDD.map(lambda line: line.split(';'))\n",
    "sqlContext = SQLContext(sc)\n",
    "dt = sqlContext.createDataFrame(parts, schema)\n",
    "to_cast_int = [\"Flight_id\", \"Days\", \"Journey_time\"]\n",
    "to_cast_double = [\"Price1_There\", \"Price1_Back\", \"Price2_There\", \"Price2_Back\", \"Full_Price\"]\n",
    "for field in to_cast_int:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(IntegerType()))\n",
    "for field in to_cast_double:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(DoubleType()))\n",
    "dt.registerTempTable(\"flights\")\n",
    "\n",
    "df = dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cofam się z datą o 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Scrap_date',  F.date_add(df['Scrap_date'], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak się cofam z datą, to mi liczba dni wzrasta o 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Days', df['Days'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomizuje predyktory mówiące o czasach odlotów i przylotów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomizacja predyktorów\n",
    "d = {'morning' : 0, 'afternoon' : 1, 'evening': 2, 'night': 3}\n",
    "inv_d = dict((v, k) for k, v in d.items())\n",
    "\n",
    "departs = ['Depart_hour1_There', 'Depart_hour2_There', 'Depart_hour1_Back', 'Depart_hour2_Back']\n",
    "arrives = ['Arrival_hour1_There', 'Arrival_hour2_There', 'Arrival_hour1_Back', 'Arrival_hour2_Back']\n",
    "\n",
    "@udf\n",
    "def random_num():\n",
    "    return random.randint(0, 3)\n",
    "\n",
    "@udf\n",
    "def subtr(depart, arrive):\n",
    "    global d\n",
    "    if (depart != 'none'):\n",
    "        return abs(d[arrive] - d[depart])\n",
    "    else:\n",
    "        return 10\n",
    "    \n",
    "@udf\n",
    "def update_time_depart(randInt):\n",
    "    global inv_d\n",
    "    return inv_d[randInt]\n",
    "\n",
    "@udf\n",
    "def update_time_arrive(randInt, subtr):\n",
    "    global inv_d\n",
    "    return inv_d[(int(randInt) + int(subtr)) % 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depart, arrive in zip(departs, arrives):\n",
    "    df = df.withColumn('randInt', random_num())\n",
    "    df = df.withColumn('subtr', subtr(df[depart] , df[arrive]))\n",
    "    df = df.withColumn(depart, F.when(df[depart] != 'none', update_time_depart(df['randInt'])).otherwise('none'))\n",
    "    df = df.withColumn(arrive, F.when(df[arrive] != 'none', update_time_arrive(df['randInt'], df['subtr'])).otherwise('none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('randInt')\n",
    "df = df.drop('subtr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updatuje cenę wg następującego algorytmu:\n",
    "1. Sprawdzam czy liczba dni jest > 90, jezeli tak to zakładam ze ten lot sie pojawil w momencie odlotu samolotu z tej samej linii lub wyprzedania biltetów. Losowo losuję days z przedziału zatem (0,10)\n",
    "2. Szukam w tabeli ze statystykami pasujących rekordów wg Country_from, Country_to oraz Scrap_time\n",
    "3. Szukam w tabeli rekord, który ma najbardziej zbliżone wyprzedzenie do zadanego\n",
    "4. Na podstawie tej tego rekordu, losuję z rozkładu normalnego o zadanej wg rekordu średniej i wariancji cenę\n",
    "5. Dodaję do niej losowy szum\n",
    "6. Zwracam wartosc ceny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafr = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_update(days):\n",
    "    if days > 90:\n",
    "        return random.randint(0, 10)\n",
    "    else:\n",
    "        return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_price(full_price, days, country_from, country_to, scrap_time, bin_size):\n",
    "    global days_stats\n",
    "    global bin_stats\n",
    "    bin_size = int(bin_size)\n",
    "    records = bin_stats[(bin_stats.Country_from == country_from) & (bin_stats.Country_to == country_to) & \n",
    "                       (bin_stats.Scrap_time == int(scrap_time))]\n",
    "    bucket = (days // bin_size) * bin_size\n",
    "    array = np.array(records['startingBin'].tolist())\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - bucket)).argmin()\n",
    "    bucket = array[idx]\n",
    "    record = records[records.startingBin == bucket]\n",
    "    # Tutaj chodzi o to, że moze wystapic jakas unikalna wartosc i wtedy nie bedzie dla niej wariancji, wtedy\n",
    "    # Ustalam wariancje na 1\n",
    "    if np.isnan(record['stdev'].item()) == False:\n",
    "        new_price = np.random.normal(loc = record['mean'], scale = record['stdev'], size = 1)[0]\n",
    "    else:\n",
    "        new_price = np.random.normal(loc = record['mean'], scale = 1, size = 1)[0]\n",
    "    new_price = new_price + np.random.rand()\n",
    "    return new_price.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafr.to_csv('test2-2019-05-14', sep = ';', header = True, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 136397/136397 [00:02<00:00, 47342.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "datafr = pd.read_csv('test2-2019-05-14', sep = ';')\n",
    "datafr['Days'] = datafr.progress_apply(lambda row : random.randint(0, 10) if (int(row[5]) > 90) else int(row[5]) , axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 136397/136397 [13:17<00:00, 171.42it/s]\n"
     ]
    }
   ],
   "source": [
    "datafr['Full_price'] = datafr.progress_apply(lambda row: update_price(row[-1], row[5], row[2], row[3], row[1], bin_size), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafr['Full_price'] = np.round(datafr['Full_price'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafr.to_csv('test2-2019-05-14', sep = ';', header = True, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Koniec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
