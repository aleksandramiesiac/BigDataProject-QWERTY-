{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zakładając, że działa to flight_id oraz mamy kolumnę w którą stronę jest lot, powiedzmy: \"Direction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj wczytanie całego pliku jako dataFrame sparka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from statistics import mean\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "import json\n",
    "\n",
    "filepath = 'flights_reduces.text'\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "filtered = flightsRDD.filter(lambda line: line != header)\n",
    "parts = filtered.map(lambda line: line.split(';'))\n",
    "flights = parts.map(lambda p:\n",
    "                   Row(Scrap_date = p[0],\n",
    "                       Scrap_time = p[1],\n",
    "                       Country_from = p[2],\n",
    "                       Country_to = p[3],\n",
    "                       Flight_id = int(p[4]),\n",
    "                       Flight_date = p[5],\n",
    "                       Airline = p[6],\n",
    "                       Change = int(p[7]),\n",
    "                       Price = float(p[8]),\n",
    "                       Depart_hour = p[9],\n",
    "                       Depart_from = p[10],\n",
    "                       Arrival_hour = p[11],\n",
    "                       Arrive_to = p[12]\n",
    "                      ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodajemy zmienną days do każdego rekordu, jaka jest różnica pomiędzy datą wylotu, a datą sprawdzenia ceny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- Arrival_hour: string (nullable = true)\n",
      " |-- Arrive_to: string (nullable = true)\n",
      " |-- Change: long (nullable = true)\n",
      " |-- Country_from: string (nullable = true)\n",
      " |-- Country_to: string (nullable = true)\n",
      " |-- Depart_from: string (nullable = true)\n",
      " |-- Depart_hour: string (nullable = true)\n",
      " |-- Flight_date: string (nullable = true)\n",
      " |-- Flight_id: long (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Scrap_date: string (nullable = true)\n",
      " |-- Scrap_time: string (nullable = true)\n",
      " |-- Days: integer (nullable = true)\n",
      " |-- Direction: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = spark.createDataFrame(flights)\n",
    "dt = dt.withColumn(\"Days\", datediff(to_date(dt.Flight_date, 'dd/MM/yy'), to_date(dt.Scrap_date, 'yyyy-MM-dd')))\n",
    "dt = dt.withColumn(\"Direction\", lit('There'))\n",
    "dt.createOrReplaceTempView(\"flights\")\n",
    "dt.printSchema()\n",
    "#dt.select('Direction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy robić to w ten sposób np. chcemy lot w jedną stronę. No to wybieramy wszystkie loty ze znacznikiem \"There\" i grupujemy cenę po id_lotu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        Price_sum|\n",
      "+-----------------+\n",
      "|4367.999999999997|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "direction = 'There'\n",
    "country_from = 'Austria'\n",
    "country_to = 'Bulgaria'\n",
    "scrap_time = '05:00'\n",
    "\n",
    "query = \"SELECT * FROM flights WHERE Country_from = '{}' AND Country_to = '{}' \\\n",
    "AND Scrap_time = '{}' AND Direction = '{}'\".format(country_from, country_to, scrap_time, direction)\n",
    "\n",
    "processed = spark.sql(query)\n",
    "processed.agg(F.sum(processed.Price).alias('Price_sum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Scrap_date: timestamp (nullable = true)\n",
      " |-- Scrap_time: string (nullable = true)\n",
      " |-- Country_from: string (nullable = true)\n",
      " |-- Country_to: string (nullable = true)\n",
      " |-- Flight_id: integer (nullable = true)\n",
      " |-- Flight_date: string (nullable = true)\n",
      " |-- Airline: string (nullable = true)\n",
      " |-- Change: integer (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Depart_hour: string (nullable = true)\n",
      " |-- Depart_from: string (nullable = true)\n",
      " |-- Arrival_hour: string (nullable = true)\n",
      " |-- Arrive_to: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Index: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"raw.csv\", sep = ';', header=True, inferSchema=True, mode = 'DROPMALFORMED') #to inferSchema to tak roboczo\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df2 = df.groupBy([\"Flight_id\"]).agg(F.first(df.Scrap_date).alias('Scrap_date'), F.first(df.Scrap_time).alias('Scrap_time'), \n",
    "                             F.sum(df.Price).alias(\"Price\"))\n",
    "df2.groupBy([\"Scrap_date\", \"Scrap_time\"]).agg(F.min(df2.Price).alias('Price')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
