{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm generowania danych:\n",
    "1. Tworzę jeden zbiór ze wszystkich dotychczasowych scrapowań\n",
    "2. Tworzę tabelę statystyk dla każdego kraju z podziałem na dni, w tym celu wykonuję następujące grupowanie i agregacje danych:\n",
    "dt.groupby(['Scrap_time', 'Country_from', 'Country_to', 'Days']).agg(\n",
    "    F.mean(dt.Full_Price),\n",
    "    F.stddev(dt.Full_Price)\n",
    ")\n",
    "Nie biorę w tym punkcie pod uwagę daty scrapowania.\n",
    "Jako wynik tego punktu otrzymyję tabelę, w której zebrane są średnia i wariancja ceny lotów na danej lini kraj-kraj, w zależności od przedziału czasu od odlotu do wylotu z podziałem na czas scrapowania.\n",
    "3. Mając taką tabelę, nazwijmy ją df, tworzę sobie krzywą, która przybliża rozkład ceny (trzeba bedzie sprawdzic, ale pewnie jakis wielomian rzedu 2), ewentualnie mogę nie tworzyć krzywej, a generować na podstawie samej tabeli (też chyba spoko)\n",
    "4. wracam do pliku z 2015-05-15.txt czyli pierwszego pliku scrapowania i biorę go jako plik referencyjny.\n",
    "5. Tworzę drugą tabelę, która jest taką zaagregowaną pierwszą, tzn. zamiast days ma biny 1-5, 6-10 itd. i z tego liczę wariancję i średnią\n",
    "4. Przechodzę po każdym wierszu tego pliku (jakaś pętla, albo coś analogicznego) i sprawdzam następujące pola:\n",
    "['Country_from', 'Country_to', 'Days', 'Scrap_time']\n",
    "Na podstawie wspomnianej tabeli liczę średnią ważoną 0.8 * df_days + 0.2 * df_bins i jako wariancję biorę wariancję z bins\n",
    "Uzasadnienie jest proste, jak biorę po dniach to nawet dla obecnych danych nie ma ich tak strasznie dużo i de facto branie wariancji po dniach może być dość zaburzoną wartością, więc lepiej chyba podzielić to na takie biny i nich patrzeć\n",
    "5. Zmieniam datę scrapowania na dzień wcześniej, Days = Days + 1, Updatuje rekord o nową cenę do której dodaję jeszcze całkiem losowy szum z rozkładu normalnego. Randomizuje godziny odlotów, przy czym z zachowaniem odpowiednich długości lotów. Tzn. robię tak:\n",
    "jak miałem lot morning - afternoon to roznica jest 1. Zatem losuje wylot na np. evening to powrot bede mial evening albo night\n",
    "6. Jak mi przekroczy 60 dni to wtedy resetuję ten lot do takiego samego z 1-dniowym wyprzedzeniem\n",
    "\n",
    "Reszty predyktorów nie zmieniam. Można też robić jakoś tak, że generować te ceny cząstkowe, ale wydaje mi się, że żeby zachować te kolumny to trzeba policzyc taki sredni udzial tych cen.\n",
    "\n",
    "W sumie mozna zrobic dwie wersje i porownac, albo wyslac do Pana Abelskiego i zapytać o opinię, które podejście jest bardziej sensowne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, lit\n",
    "from pyspark.sql.types import *#IntegerType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf = SparkConf().setAppName('MyFirstStandaloneApp')\n",
    "sc = SparkContext(conf=conf)\n",
    "#sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@udf\n",
    "#def groupped_days(days):\n",
    "#    if (days % 3 != 0):\n",
    "#        low = days // 3 * 3 + 1\n",
    "#        up = low + 2\n",
    "#    else:\n",
    "#        up = 3 * (days // 3)\n",
    "#    return(str(up - 2) + '-' + str(up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def groupped_days(days, bin_size):\n",
    "    bin_size = int(bin_size)\n",
    "    low = days // bin_size * bin_size\n",
    "    up = low + bin_size - 1\n",
    "    return(str(low) + '-' + str(up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Structured_data2\"\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "flightsRDD = flightsRDD.filter(lambda line : line != header)\n",
    "colnames = header.split(';')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)\n",
    "parts = flightsRDD.map(lambda line: line.split(';'))\n",
    "sqlContext = SQLContext(sc)\n",
    "dt = sqlContext.createDataFrame(parts, schema)\n",
    "to_cast_int = [\"Flight_id\", \"Days\", \"Journey_time\"]\n",
    "to_cast_double = [\"Price1_There\", \"Price1_Back\", \"Price2_There\", \"Price2_Back\", \"Full_Price\"]\n",
    "for field in to_cast_int:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(IntegerType()))\n",
    "for field in to_cast_double:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(DoubleType()))\n",
    "dt.registerTempTable(\"flights\")\n",
    "\n",
    "df = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekDay =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d').strftime('%w'))\n",
    "\n",
    "df = dt.withColumn('weekDay', weekDay(dt['Scrap_date']))\n",
    "df = df.withColumn('grouppedDays', groupped_days(df['Days'], lit(bin_size)))\n",
    "fun = udf(lambda x : int(x.split('-')[0]))\n",
    "df = df.withColumn('startingBin', fun(df.grouppedDays))\n",
    "#df = df.withColumn('daysToLeave', df['Days'] + df.['Journey_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj tworzenie tabel statystycznych do generowania danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(['Scrap_time', 'Country_from', 'Country_to', 'startingBin']).agg(\n",
    "F.mean(df.Full_Price).alias('mean'),\n",
    "F.stddev(df.Full_Price).alias('stdev'),\n",
    "F.count(df.Full_Price).alias('count'))\n",
    "\n",
    "df2 = df.groupby(['Scrap_time', 'Country_from', 'Country_to', 'Days']).agg(\n",
    "F.mean(df.Full_Price).alias('mean'),\n",
    "F.stddev(df.Full_Price).alias('stdev'),\n",
    "F.count(df.Full_Price).alias('count'))\n",
    "\n",
    "days_stats = df2.toPandas()\n",
    "bin_stats = df1.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uporządkowanie typów w dataframe'ach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(obj, datastructure):\n",
    "    return any([obj == elem for elem in datastructure])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "asint = ['Scrap_time', 'startingBin', 'count', 'Days']\n",
    "asfloat = ['mean', 'stdev']\n",
    "day_col = list(days_stats.columns)\n",
    "bin_col = list(bin_stats.columns)\n",
    "\n",
    "for c1, c2 in zip(day_col, bin_col):\n",
    "    if exists(c1 , day_col):\n",
    "        if exists(c1, asint):\n",
    "            days_stats[c1] = days_stats[c1].astype('int')\n",
    "        elif exists(c1, asfloat):\n",
    "            days_stats[c1] = days_stats[c1].astype('float')\n",
    "    if exists(c2 , bin_col):\n",
    "        if exists(c2, asint):\n",
    "            bin_stats[c2] = bin_stats[c2].astype('int')\n",
    "        elif exists(c2, asfloat):\n",
    "            bin_stats[c2] = bin_stats[c2].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj właściwe generowanie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie pliku referencyjnego, czyli pliku z najstarszego scrapowania: 2019-05-15.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Reference_file\"\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "flightsRDD = flightsRDD.filter(lambda line : line != header)\n",
    "colnames = header.split(';')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)\n",
    "parts = flightsRDD.map(lambda line: line.split(';'))\n",
    "sqlContext = SQLContext(sc)\n",
    "dt = sqlContext.createDataFrame(parts, schema)\n",
    "to_cast_int = [\"Flight_id\", \"Days\", \"Journey_time\"]\n",
    "to_cast_double = [\"Price1_There\", \"Price1_Back\", \"Price2_There\", \"Price2_Back\", \"Full_Price\"]\n",
    "for field in to_cast_int:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(IntegerType()))\n",
    "for field in to_cast_double:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(DoubleType()))\n",
    "dt.registerTempTable(\"flights\")\n",
    "\n",
    "df = dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generuję cofam się z datą o 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Scrap_date',  F.date_add(df['Scrap_date'], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak się cofam z datą, to mi liczba dni wzrasta o 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Days', df['Days'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updatuje cenę wg następującego algorytmu:\n",
    "1. Sprawdzam czy liczba dni jest > 90, jezeli tak to zakładam ze ten lot sie pojawil w momencie odlotu samolotu z tej samej linii lub wyprzedania biltetów. Losowo losuję days z przedziału zatem (0,10)\n",
    "2. Szukam w tabeli ze statystykami pasujących rekordów wg Country_from, Country_to oraz Scrap_time\n",
    "3. Szukam w tabeli rekord, który ma najbardziej zbliżone wyprzedzenie do zadanego\n",
    "4. Na podstawie tej tego rekordu, losuję z rozkładu normalnego o zadanej wg rekordu średniej i wariancji cenę\n",
    "5. Dodaję do niej losowy szum\n",
    "6. Zwracam wartosc ceny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def update_price(full_price, days, country_from, country_to, scrap_time, bin_size):\n",
    "    global days_stats\n",
    "    global bin_stats\n",
    "    bin_size = int(bin_size)\n",
    "    if days > 90:\n",
    "        days = random.randint(0, 10)\n",
    "    records = bin_stats[(bin_stats.Country_from == country_from) & (bin_stats.Country_to == country_to) & \n",
    "                       (bin_stats.Scrap_time == int(scrap_time))]\n",
    "    bucket = (days // bin_size) * bin_size\n",
    "    array = np.array(records['startingBin'].tolist())\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - bucket)).argmin()\n",
    "    bucket = array[idx]\n",
    "    record = records[records.startingBin == bucket]\n",
    "    new_price = np.random.normal(loc = record['mean'], scale = record['stdev'], size = 1)[0]\n",
    "    new_price = new_price + np.random.rand()\n",
    "    return new_price.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie randomizuje predyktory mówiące o czasach odlotów i przylotów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomizacja predyktorów\n",
    "d = {'morning' : 0, 'afternoon' : 1, 'evening': 2, 'night': 3}\n",
    "inv_d = dict((v, k) for k, v in d.items())\n",
    "\n",
    "departs = ['Depart_hour1_There', 'Depart_hour2_There', 'Depart_hour1_Back', 'Depart_hour2_Back']\n",
    "arrives = ['Arrival_hour1_There', 'Arrival_hour2_There', 'Arrival_hour1_Back', 'Arrival_hour2_Back']\n",
    "\n",
    "@udf\n",
    "def random_num():\n",
    "    return random.randint(0, 3)\n",
    "\n",
    "@udf\n",
    "def subtr(depart, arrive):\n",
    "    global d\n",
    "    if (depart != 'none'):\n",
    "        return abs(d[arrive] - d[depart])\n",
    "    else:\n",
    "        return 10\n",
    "    \n",
    "@udf\n",
    "def update_time_depart(randInt):\n",
    "    global inv_d\n",
    "    return inv_d[randInt]\n",
    "\n",
    "@udf\n",
    "def update_time_arrive(randInt, subtr):\n",
    "    global inv_d\n",
    "    return inv_d[(randInt + subtr) % 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4 + 1) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'morning', 1: 'afternoon', 2: 'evening', 3: 'night'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "for depart, arrive in zip(departs, arrives):\n",
    "    df = df.withColumn('randInt', random_num())\n",
    "    df = df.withColumn('subtr', subtr(df[depart] , df[arrive]))\n",
    "    df = df.withColumn(depart, F.when(df[depart] != 'none', update_time_depart(df['randInt'])).otherwise('none'))\n",
    "    #df = df.withColumn(arrive, F.when(df[arrive] != 'none', update_time_arrive(df['randInt'], df['subtr'])).otherwise('none'))\n",
    "    # Ta zakomentowana linijka się sypie i nie wiem dlaczego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3333.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 857, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 85, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-133-e81bc4959274>\", line 28, in update_time_arrive\nTypeError: not all arguments converted during string formatting\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.GeneratedMethodAccessor81.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 85, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-133-e81bc4959274>\", line 28, in update_time_arrive\nTypeError: not all arguments converted during string formatting\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-1a6ce2362cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \"\"\"\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3333.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 1 times, most recent failure: Lost task 0.0 in stage 41.0 (TID 857, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 85, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-133-e81bc4959274>\", line 28, in update_time_arrive\nTypeError: not all arguments converted during string formatting\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.GeneratedMethodAccessor81.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 85, in <lambda>\n  File \"C:\\Users\\piotr\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-133-e81bc4959274>\", line 28, in update_time_arrive\nTypeError: not all arguments converted during string formatting\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Jak show zadziała to znaczy, ze jest wszystko ok\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poniżej jakieś moje eksperymenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|subtr|randInt|\n",
      "+-----+-------+\n",
      "|    0|      3|\n",
      "|    0|      3|\n",
      "|    0|      1|\n",
      "|   10|      0|\n",
      "|    0|      3|\n",
      "|   10|      3|\n",
      "|   10|      3|\n",
      "|   10|      2|\n",
      "|   10|      0|\n",
      "|   10|      0|\n",
      "|   10|      1|\n",
      "|    0|      1|\n",
      "|    0|      0|\n",
      "|   10|      3|\n",
      "|   10|      3|\n",
      "|    0|      3|\n",
      "|    0|      3|\n",
      "|   10|      2|\n",
      "|    0|      2|\n",
      "|    1|      0|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['subtr', 'randInt']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scrap_time', 'Country_from', 'Country_to', 'Days', 'mean', 'stdev', 'count']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(days_stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scrap_time</th>\n",
       "      <th>Country_from</th>\n",
       "      <th>Country_to</th>\n",
       "      <th>Days</th>\n",
       "      <th>mean</th>\n",
       "      <th>stdev</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>34</td>\n",
       "      <td>76.427000</td>\n",
       "      <td>23.122924</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>33</td>\n",
       "      <td>83.011250</td>\n",
       "      <td>119.180928</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Austria</td>\n",
       "      <td>Poland</td>\n",
       "      <td>254</td>\n",
       "      <td>33.205714</td>\n",
       "      <td>10.653705</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>Austria</td>\n",
       "      <td>166</td>\n",
       "      <td>64.990000</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>England</td>\n",
       "      <td>Austria</td>\n",
       "      <td>20</td>\n",
       "      <td>42.387500</td>\n",
       "      <td>4.395943</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scrap_time Country_from Country_to  Days       mean       stdev  count\n",
       "0          8      Croatia    Belgium    34  76.427000   23.122924     10\n",
       "1         16       Norway    Belgium    33  83.011250  119.180928      8\n",
       "2         20      Austria     Poland   254  33.205714   10.653705      7\n",
       "3         20      Denmark    Austria   166  64.990000    0.011547      4\n",
       "4          9      England    Austria    20  42.387500    4.395943     24"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scrap_time</th>\n",
       "      <th>Country_from</th>\n",
       "      <th>Country_to</th>\n",
       "      <th>startingBin</th>\n",
       "      <th>mean</th>\n",
       "      <th>stdev</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Austria</td>\n",
       "      <td>France</td>\n",
       "      <td>0</td>\n",
       "      <td>45.440000</td>\n",
       "      <td>8.192647</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>265</td>\n",
       "      <td>53.835385</td>\n",
       "      <td>1.720173</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>5</td>\n",
       "      <td>35.270952</td>\n",
       "      <td>4.188281</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Poland</td>\n",
       "      <td>170</td>\n",
       "      <td>32.898214</td>\n",
       "      <td>8.568113</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>Norway</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>5</td>\n",
       "      <td>64.071266</td>\n",
       "      <td>3.634045</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Scrap_time Country_from Country_to startingBin       mean     stdev  count\n",
       "0          8      Austria     France           0  45.440000  8.192647     15\n",
       "1         17       Poland    Ireland         265  53.835385  1.720173     13\n",
       "2         17        Spain    Belgium           5  35.270952  4.188281     84\n",
       "3         22        Italy     Poland         170  32.898214  8.568113     28\n",
       "4         22       Norway    Ireland           5  64.071266  3.634045     79"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_stats[(days_stats.Country_from == 'Poland') & (days_stats.Country_to == 'Ireland') & \n",
    "           (days_stats.Days == 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_from = 'Poland'\n",
    "country_to = 'Austria'\n",
    "days = 400\n",
    "scrap_time = 10\n",
    "bin_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scrap_time</th>\n",
       "      <th>Country_from</th>\n",
       "      <th>Country_to</th>\n",
       "      <th>startingBin</th>\n",
       "      <th>avg(Full_Price)</th>\n",
       "      <th>stddev_samp(Full_Price)</th>\n",
       "      <th>count(Full_Price)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>10</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Austria</td>\n",
       "      <td>170</td>\n",
       "      <td>47.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>4</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Austria</td>\n",
       "      <td>260</td>\n",
       "      <td>27.688235</td>\n",
       "      <td>11.058018</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>10</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Austria</td>\n",
       "      <td>25</td>\n",
       "      <td>44.211143</td>\n",
       "      <td>10.068659</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>22</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Austria</td>\n",
       "      <td>105</td>\n",
       "      <td>45.858125</td>\n",
       "      <td>9.338752</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>10</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Austria</td>\n",
       "      <td>195</td>\n",
       "      <td>44.318000</td>\n",
       "      <td>2.475716</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Scrap_time Country_from Country_to startingBin  avg(Full_Price)  \\\n",
       "137         10       Poland    Austria         170        47.400000   \n",
       "337          4       Poland    Austria         260        27.688235   \n",
       "630         10       Poland    Austria          25        44.211143   \n",
       "644         22       Poland    Austria         105        45.858125   \n",
       "704         10       Poland    Austria         195        44.318000   \n",
       "\n",
       "     stddev_samp(Full_Price)  count(Full_Price)  \n",
       "137                      NaN                  1  \n",
       "337                11.058018                 17  \n",
       "630                10.068659                 35  \n",
       "644                 9.338752                 16  \n",
       "704                 2.475716                  5  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_stats[(bin_stats.Country_from == country_from) & (bin_stats.Country_to == country_to)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =bin_stats[(bin_stats.Country_from == country_from) & (bin_stats.Country_to == country_to) & (bin_stats.startingBin == (days // bin_size) * bin_size) & (bin_stats.Scrap_time == scrap_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(days // bin_size) * bin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_stats[(bin_stats.Country_from == 'Poland') & (bin_stats.Country_to == 'Ireland') & (bin_stats.startingBin == '10')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj to właściwe generowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"Reference_file\"\n",
    "\n",
    "flightsRDD = sc.textFile(filepath)\n",
    "header = flightsRDD.first()\n",
    "flightsRDD = flightsRDD.filter(lambda line : line != header)\n",
    "colnames = header.split(';')\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in colnames]\n",
    "schema = StructType(fields)\n",
    "parts = flightsRDD.map(lambda line: line.split(';'))\n",
    "sqlContext = SQLContext(sc)\n",
    "dt = sqlContext.createDataFrame(parts, schema)\n",
    "to_cast_int = [\"Flight_id\", \"Days\", \"Journey_time\"]\n",
    "to_cast_double = [\"Price1_There\", \"Price1_Back\", \"Price2_There\", \"Price2_Back\", \"Full_Price\"]\n",
    "for field in to_cast_int:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(IntegerType()))\n",
    "for field in to_cast_double:\n",
    "    dt = dt.withColumn(field,  dt[field].cast(DoubleType()))\n",
    "dt.registerTempTable(\"flights\")\n",
    "\n",
    "df = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cofnij sie z data o 1\n",
    "df = df.withColumn('Scrap_date',  F.date_add(df['Scrap_date'], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wzrost liczby dni o 1\n",
    "df = df.withColumn('Days', df['Days'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "?random.randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Scrap_date: date (nullable = true)\n",
      " |-- Scrap_time: string (nullable = true)\n",
      " |-- Country_from: string (nullable = true)\n",
      " |-- Country_to: string (nullable = true)\n",
      " |-- Flight_id: integer (nullable = true)\n",
      " |-- Days: integer (nullable = true)\n",
      " |-- Journey_time: integer (nullable = true)\n",
      " |-- Airline1_There: string (nullable = true)\n",
      " |-- Airline1_Back: string (nullable = true)\n",
      " |-- Airline2_There: string (nullable = true)\n",
      " |-- Airline2_Back: string (nullable = true)\n",
      " |-- Price1_There: double (nullable = true)\n",
      " |-- Price1_Back: double (nullable = true)\n",
      " |-- Price2_There: double (nullable = true)\n",
      " |-- Price2_Back: double (nullable = true)\n",
      " |-- Depart_hour1_There: string (nullable = true)\n",
      " |-- Depart_hour1_Back: string (nullable = true)\n",
      " |-- Depart_hour2_There: string (nullable = true)\n",
      " |-- Depart_hour2_Back: string (nullable = true)\n",
      " |-- Depart_from1_There: string (nullable = true)\n",
      " |-- Depart_from1_Back: string (nullable = true)\n",
      " |-- Depart_from2_There: string (nullable = true)\n",
      " |-- Depart_from2_Back: string (nullable = true)\n",
      " |-- Arrival_hour1_There: string (nullable = true)\n",
      " |-- Arrival_hour1_Back: string (nullable = true)\n",
      " |-- Arrival_hour2_There: string (nullable = true)\n",
      " |-- Arrival_hour2_Back: string (nullable = true)\n",
      " |-- Arrive_to1_There: string (nullable = true)\n",
      " |-- Arrive_to1_Back: string (nullable = true)\n",
      " |-- Arrive_to2_There: string (nullable = true)\n",
      " |-- Arrive_to2_Back: string (nullable = true)\n",
      " |-- Full_Price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def update_price(full_price, days, country_from, country_to, scrap_time, bin_size):\n",
    "    global days_stats\n",
    "    global bin_stats\n",
    "    bin_size = int(bin_size)\n",
    "    #if days > 90:\n",
    "    #    days = random.randint(0, 10)\n",
    "    records = bin_stats[(bin_stats.Country_from == country_from) & (bin_stats.Country_to == country_to) & \n",
    "                       (bin_stats.Scrap_time == int(scrap_time))]\n",
    "    bucket = (days // bin_size) * bin_size\n",
    "    array = np.array(records['startingBin'].tolist())\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - bucket)).argmin()\n",
    "    bucket = array[idx]\n",
    "    record = records[records.startingBin == bucket]\n",
    "    new_price = np.random.normal(loc = record['mean'], scale = record['stdev'], size = 1)[0]\n",
    "    new_price = new_price + np.random.rand()\n",
    "    return new_price.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ceny Full_price\n",
    "a = df.withColumn('Full_price', update_price(df['Full_Price'], df['Days'], df['Country_from'],\n",
    "                                             df['Country_to'], df['Scrap_time'], lit(bin_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_price = np.random.normal(loc = 0, scale = 1, size = 1)[0]\n",
    "new_price = new_price + np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5869450041147517"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_price.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+----------+---------+----+------------+--------------+-------------+--------------+-------------+------------+-----------+------------+-----------+------------------+-----------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|Scrap_date|Scrap_time|Country_from|Country_to|Flight_id|Days|Journey_time|Airline1_There|Airline1_Back|Airline2_There|Airline2_Back|Price1_There|Price1_Back|Price2_There|Price2_Back|Depart_hour1_There|Depart_hour1_Back|Depart_hour2_There|Depart_hour2_Back|  Depart_from1_There|   Depart_from1_Back|  Depart_from2_There|   Depart_from2_Back|Arrival_hour1_There|Arrival_hour1_Back|Arrival_hour2_There|Arrival_hour2_Back|    Arrive_to1_There|     Arrive_to1_Back|    Arrive_to2_There|     Arrive_to2_Back|        Full_price|\n",
      "+----------+----------+------------+----------+---------+----+------------+--------------+-------------+--------------+-------------+------------+-----------+------------+-----------+------------------+-----------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|2019-05-14|         2|     Austria|   Austria|       71| 148|           8|   Laudamotion|  Laudamotion|   Laudamotion|  Laudamotion|       24.99|      24.99|       24.99|      24.99|         afternoon|          morning|           evening|        afternoon|        Salzburg SZG|          Vienna VIE|      Palma Mall PMI|      Palma Mall PMI|          afternoon|           morning|            evening|         afternoon|Palma Mall PMI OE542|Palma Mall PMI OE502|          Vienna VIE|        Salzburg SZG|148.85867343289706|\n",
      "|2019-05-14|         2|     Austria|   Belgium|      265|   3|           7|   Laudamotion|    Jetairfly|     Jetairfly|  Laudamotion|       93.83|      39.99|       39.99|       9.99|         afternoon|          morning|           evening|        afternoon|          Vienna VIE|Ostend OSTOstend-...|      Palma Mall PMI|       Heraklion HER|          afternoon|         afternoon|            evening|         afternoon|Palma Mall PMI OE122|Heraklion HER TB2281|Ostend OSTOstend-...|          Vienna VIE|240.39025847886563|\n",
      "|2019-05-14|         2|     Austria|   Croatia|      345|  42|           8|   Laudamotion|      easyJet|       Volotea|      easyJet|       14.99|       7.66|       26.72|      13.67|           morning|          morning|           evening|          evening|          Vienna VIE|           Split SPU|Milan BGYMilan (B...|Berlin SXFBerlin ...|            morning|           morning|            evening|           evening|Milan BGYMilan (B...|Berlin SXFBerlin ...|           Split SPU|          Vienna VIE| 76.45394651129027|\n",
      "|2019-05-14|         2|     Austria|   Denmark|      526|  23|           6|   Laudamotion|     Wizz Air|       Ryanair|         none|       16.83|      10.58|       19.99|        0.0|           morning|        afternoon|         afternoon|             none|          Vienna VIE|         Billund BLL|Milan BGYMilan (B...|                none|            morning|         afternoon|          afternoon|              none|Milan BGYMilan (B...|          Vienna VIE|         Billund BLL|                none|  29.0616100309054|\n",
      "|2019-05-14|         2|     Austria|   England|      659|  22|           8|   Laudamotion|      Ryanair|          none|     Wizz Air|       19.99|      10.36|         0.0|      14.99|           morning|          morning|              none|          evening|          Vienna VIE|London STNLondon ...|                none|Milan MXPMilan (M...|            morning|           morning|               none|           evening|London STNLondon ...|Milan MXPMilan (M...|                none|          Vienna VIE| 40.65285929273961|\n",
      "|2019-05-14|         2|     Austria|    France|      839|  41|           6|   Laudamotion|  Laudamotion|          none|         none|       19.99|      19.99|         0.0|        0.0|         afternoon|        afternoon|              none|             none|          Vienna VIE|Paris BVAParis (B...|                none|                none|          afternoon|         afternoon|               none|              none|Paris BVAParis (B...|          Vienna VIE|                none|                none|55.168768395553364|\n",
      "|2019-05-14|         2|     Austria|   Germany|     1105| 194|           8|       easyJet|      easyJet|          none|         none|        4.99|      22.37|         0.0|        0.0|           evening|        afternoon|              none|             none|       Innsbruck INN|Berlin TXLBerlin ...|                none|                none|            evening|         afternoon|               none|              none|Berlin TXLBerlin ...|       Innsbruck INN|                none|                none|27.791630331239865|\n",
      "|2019-05-14|         2|     Austria|    Greece|     1293|  19|           7|   Laudamotion|    Eurowings|          none|         none|       24.99|      49.99|         0.0|        0.0|             night|            night|              none|             none|          Vienna VIE|           Corfu CFU|                none|                none|            morning|             night|               none|              none|           Corfu CFU|          Vienna VIE|                none|                none| 35.53488000585466|\n",
      "|2019-05-14|         2|     Austria|     Italy|     1601|  21|           8|   Laudamotion|  Laudamotion|   Laudamotion|         none|        9.99|       9.99|        9.99|        0.0|           morning|        afternoon|           evening|             none|          Vienna VIE|         Bologna BLQ|       Stuttgart STR|                none|            morning|         afternoon|            evening|              none| Stuttgart STR OE620|          Vienna VIE|         Bologna BLQ|                none|28.419971526647025|\n",
      "|2019-05-14|         2|     Austria|    Norway|     1768| 153|           4|      Wizz Air|     Wizz Air|          none|         none|       34.99|      35.65|         0.0|        0.0|         afternoon|          evening|              none|             none|          Vienna VIE|          Bergen BGO|                none|                none|            evening|           evening|               none|              none|          Bergen BGO|          Vienna VIE|                none|                none| 81.53154692545154|\n",
      "|2019-05-14|         2|     Austria|    Poland|     2012|  44|           6|   Laudamotion|     Wizz Air|      Wizz Air|         none|        9.99|      17.41|       21.85|        0.0|         afternoon|          morning|         afternoon|             none|          Vienna VIE|          Warsaw WAW|Bucharest OTPBuch...|                none|          afternoon|         afternoon|          afternoon|              none|Bucharest OTPBuch...|          Vienna VIE|          Warsaw WAW|                none| 42.60465054053556|\n",
      "|2019-05-14|         2|     Austria|  Portugal|     2160| 188|           4|   Laudamotion|      easyJet|          none|      easyJet|       30.59|       7.99|         0.0|      23.45|         afternoon|          morning|              none|          evening|          Vienna VIE|           Porto OPO|                none|            Lyon LYS|            evening|         afternoon|               none|           evening|           Porto OPO|     Lyon LYS U24442|                none|          Vienna VIE| 63.56199618991374|\n",
      "|2019-05-14|         2|     Austria|    Russia|     2327| 146|           5|   Laudamotion|      Vueling|       Vueling|  Laudamotion|       30.59|      94.99|      104.99|      36.71|           evening|            night|           evening|          morning|          Vienna VIE|      St Petersb LED|       Barcelona BCN|       Barcelona BCN|            evening|           morning|              night|           morning|Barcelona BCN OE1316|Barcelona BCN VY7893|      St Petersb LED|          Vienna VIE| 281.0254434610148|\n",
      "|2019-05-14|         2|     Belgium|   Austria|     2601| 177|           7|       Ryanair|  Laudamotion|       easyJet|         none|       24.99|      23.86|       14.49|        0.0|           morning|          morning|         afternoon|             none|Brussels CRLBruss...|          Vienna VIE|          Naples NAP|                none|            morning|           morning|          afternoon|              none|   Naples NAP FR1302|Brussels CRLBruss...|          Vienna VIE|                none| 72.68954482848652|\n",
      "|2019-05-14|         2|     Belgium|   Croatia|     2779|   4|           7|       Ryanair|      Ryanair|          none|         none|       14.99|      14.99|         0.0|        0.0|         afternoon|        afternoon|              none|             none|Brussels CRLBruss...|           Zadar ZAD|                none|                none|          afternoon|         afternoon|               none|              none|           Zadar ZAD|Brussels CRLBruss...|                none|                none|36.569937651261164|\n",
      "|2019-05-14|         2|     Belgium|   Denmark|     3056| 153|           4|       Ryanair|      easyJet|       easyJet|      Ryanair|       19.99|      12.28|       15.69|      19.99|           morning|        afternoon|         afternoon|          evening|        Brussels BRU|      Copenhagen CPH|Berlin SXFBerlin ...|Berlin SXFBerlin ...|            morning|         afternoon|          afternoon|           evening|Berlin SXFBerlin ...|Berlin SXFBerlin ...|      Copenhagen CPH|        Brussels BRU| 53.80919106787022|\n",
      "|2019-05-14|         2|     Belgium|   England|     3189|   8|           7|       Ryanair|      Ryanair|       Ryanair|      Ryanair|       22.98|      14.97|       12.99|      14.99|           morning|        afternoon|         afternoon|          evening|        Brussels BRU|         Bristol BRS|          Dublin DUB|Milan MXPMilan (M...|            morning|           evening|          afternoon|           evening|   Dublin DUB FR1453|Milan MXPMilan (M...|         Bristol BRS|        Brussels BRU| 35.06795195033788|\n",
      "|2019-05-14|         2|     Belgium|    France|     3369|  28|           4|       Ryanair|      Ryanair|          none|         none|       17.99|      19.99|         0.0|        0.0|           morning|        afternoon|              none|             none|Brussels CRLBruss...|           Nimes FNI|                none|                none|            morning|           evening|               none|              none|           Nimes FNI|Brussels CRLBruss...|                none|                none|  35.7339675509682|\n",
      "|2019-05-14|         2|     Belgium|    Greece|     3727| 159|           4|       Ryanair|      Ryanair|          none|      Ryanair|       24.99|      29.37|         0.0|      17.14|         afternoon|        afternoon|              none|          evening|Brussels CRLBruss...|           Corfu CFU|                none|        Budapest BUD|            evening|         afternoon|               none|           evening|           Corfu CFU| Budapest BUD FR8053|                none|Brussels CRLBruss...|60.502238640720904|\n",
      "|2019-05-14|         2|     Belgium|   Ireland|     3983|   1|           6|         Flybe|      Ryanair|       Ryanair|        Flybe|       39.99|      12.99|       14.97|       46.1|         afternoon|          morning|           evening|          morning|Antwerp ANRAntwer...|          Dublin DUB|London SENLondon ...|London SENLondon ...|          afternoon|           morning|            evening|         afternoon|London SENLondon ...|London SENLondon ...|          Dublin DUB|Antwerp ANRAntwer...|62.789535523462135|\n",
      "+----------+----------+------------+----------+---------+----+------------+--------------+-------------+--------------+-------------+------------+-----------+------------+-----------+------------------+-----------------+------------------+-----------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+-------------------+------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomizacja predyktorów\n",
    "d = {'morning' : 0, 'afternoon' : 1, 'evening': 2, 'night': 3}\n",
    "inv_d = dict((v, k) for k, v in d.items())\n",
    "\n",
    "theres = ['Depart_hour1_There', 'Depart_hour2_There', 'Arrival_hour1_There', 'Arrival_hour2_There']\n",
    "backs = ['Depart_hour1_Back', 'Depart_hour2_Back', 'Arrival_hour1_Back', 'Arrival_hour2_Back']\n",
    "\n",
    "@udf\n",
    "def random_num():\n",
    "    return np.random.randint(0, 3)\n",
    "\n",
    "@udf\n",
    "def subtr(there, back):\n",
    "    global d\n",
    "    if (there != 'null'):\n",
    "        return abs(d[back] - d[there])\n",
    "    else:\n",
    "        return 10\n",
    "    \n",
    "@udf\n",
    "def update_time_there(filed, num):\n",
    "    global inv_d\n",
    "    return \n",
    "\n",
    "@udf\n",
    "def update_time_back(filed, num):\n",
    "    global inv_d\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for there, back in zip(theres, backs):\n",
    "    df.withColumn('randInt', random_num())\n",
    "    df.withColumn('subtr', subtr(df[there] , df[back]))\n",
    "    df.withColumn(there, F.when(df[there] != 'null', inv_d(df['randInt'])).otherwise('null')\n",
    "    df.withColumn(back, F.when(df[back] != 'null', inv_d(df['randInt'] + df['subtr'] % 4)).otherwise('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cofnij sie z data o 1\n",
    "df = df.withColumn('Scrap_date',  F.date_add(df['Scrap_date'], -1))\n",
    "# Wzrost liczby dni o 1\n",
    "df = df.withColumn('Days', df['Days'] + 1)\n",
    "# Update ceny Full_price\n",
    "df = df.withColumn('Full_price', update_price(df['Full_Price'], df['Days'], df['Country_from'],\n",
    "                                             df['Country_to'], df['Scrap_time'], lit(bin_size)))\n",
    "# Randomizacja predyktorów\n",
    "d = {'morning' : 0, 'afternoon' : 1, 'evening': 2, 'night': 3}\n",
    "inv_d = dict((v, k) for k, v in d.items())\n",
    "\n",
    "theres = ['Depart_hour1_There', 'Depart_hour2_There', 'Arrival_hour1_There', 'Arrival_hour2_There']\n",
    "backs = ['Depart_hour1_Back', 'Depart_hour2_Back', 'Arrival_hour1_Back', 'Arrival_hour2_Back']\n",
    "\n",
    "@udf\n",
    "def random_num():\n",
    "    return np.random.randint(0, 3)\n",
    "\n",
    "@udf\n",
    "def subtr(there, back):\n",
    "    global d\n",
    "    if (there != 'null'):\n",
    "        return abs(d[back] - d[there])\n",
    "    else:\n",
    "        return 10\n",
    "    \n",
    "@udf\n",
    "def update_time_there(filed, num):\n",
    "    global inv_d\n",
    "    return \n",
    "\n",
    "@udf\n",
    "def update_time_back(filed, num):\n",
    "    global inv_d\n",
    "    return \n",
    "\n",
    "for there, back in zip(theres, backs):\n",
    "    df.withColumn('randInt', random_num())\n",
    "    df.withColumn('subtr', subtr(df[there] , df[back]))\n",
    "    df.withColumn(there, F.when(df[there] != 'null', inv_d(df['randInt'])).otherwise('null')\n",
    "    df.withColumn(back, F.when(df[back] != 'null', inv_d(df['randInt'] + df['subtr'] % 4)).otherwise('null')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_price(full_price, days, country_from, country_to, scrap_time, bin_size):\n",
    "    global days_stats\n",
    "    global bins_stats\n",
    "    bin_size = int(bin_size)\n",
    "    record = bins_stats[(bins_stats.Country_from == country_from) & (bins_stats.Country_to == country_to) & \n",
    "           (bins_stats.startingBin == days // bin_size * bin_size) & (bins_stats.Scrap_time == scrap_time)]\n",
    "    new_price = np.random.normal(loc = record['avg(Full_Price)'], scale = record['stddev_samp(Full_Price)'], size = 1)\n",
    "    new_price = new_price + np.random.rand()\n",
    "    return new_price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def randomize_time(there, back):\n",
    "    d = dict()\n",
    "    d['morning'] = 0\n",
    "    d['afternoon'] = 1\n",
    "    d['evening'] = 2\n",
    "    d['night'] = 3\n",
    "    inv = dict((v, k) for k, v in d.items()) # iteritems for Python 2.7\n",
    "    subtr = abs(d[back] - d[there])\n",
    "    t = random.randint(0, 3)\n",
    "    return (inv[t], inv[(t + subtr) % 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomize_time('morning', 'afternoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.random.normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(['Basia', 'Kasia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = copy.deepcopy(datafr)\n",
    "#cp = cp[(cp.Country_from == 'France') & (cp.Country_to == 'Greece')]\n",
    "#cp = cp.loc[cp.startingBin.astype(float).sort_values(ascending=True).index]\n",
    "result = cp.groupby(['Scrap_time', 'startingBin']).agg({'avg(Full_Price)': 'mean', 'count(Full_Price)': 'sum'})\n",
    "result['startBin'] = result.index\n",
    "result = result.loc[result.startBin.astype(float).sort_values(ascending=True).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(num = 1, figsize = (10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.array([int(s) for s in list(result['startBin'])])\n",
    "y = np.array(result['avg(Full_Price)'])\n",
    "p1 = np.polyfit(x, y, 2)\n",
    "y1 = np.polyval(p1, x)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, y1)\n",
    "plt.subplot(1,2,2)\n",
    "x = x[x < 60]\n",
    "y = y[0:x.shape[0]]\n",
    "p2 = np.polyfit(x, y, 1)\n",
    "y2 = np.polyval(p2, x)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cofnij sie z data o 1\n",
    "df = df.withColumn('Scrap_date',  F.date_add(df['Scrap_date'], -1))\n",
    "# Wzrost liczby dni o 1\n",
    "df = df.withColumn('Days', df['Days'] + 1)\n",
    "# Update ceny Full_price\n",
    "df = df.withColumn('Full_price', update_price(df['Full_Price'], df['Days'], df['Country_from'],\n",
    "                                             df['Country_to'], df['Scrap_time']))\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_stats\n",
    "bins_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_price(full_price, days, country_from, country_to, scrap_time):\n",
    "    global days_stats\n",
    "    global bins_stats\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeFmt1 = \"yyyy-MM-dd\"\n",
    "timeFmt2 = \"dd/MM/yy\"\n",
    "timeDiff = ((F.unix_timestamp('Flight_date', format=timeFmt2)\n",
    "            - F.unix_timestamp('Scrap_date', format=timeFmt1)) / (24 * 3600)).cast(IntegerType())\n",
    "dt = dt.withColumn(\"Days\", timeDiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "weekDay =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d').strftime('%w'))\n",
    "\n",
    "df = dt.withColumn('weekDay', weekDay(dt['Scrap_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uwaga, Python jest bardzo chrześcijański i numeruje dni tygodnia od niedzieli, zatem weekDay: 0 to niedziela\n",
    "\n",
    "dat = '2019-05-26'\n",
    "w = lambda x: datetime.strptime(x, '%Y-%m-%d').strftime('%w')\n",
    "w(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def groupped_days(days):\n",
    "    if (days % 3 != 0):\n",
    "        low = days // 3 * 3 + 1\n",
    "        up = low + 2\n",
    "    else:\n",
    "        up = 3 * (days // 3)\n",
    "    return(str(up - 2) + '-' + str(up))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('grouppedDays', groupped_days(df['Days']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = udf(lambda x : x.split('-')[0])\n",
    "df = df.withColumn('startingBin', fun(df.grouppedDays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['Scrap_time', 'Country_from', 'Country_to', 'startingBin']).agg(\n",
    "F.mean(df.Full_Price),\n",
    "F.stddev(df.Full_Price),\n",
    "F.count(df.Full_Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "datafr = df.toPandas()\n",
    "datafr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = copy.deepcopy(datafr)\n",
    "cp = cp[(cp.Country_from == 'France') & (cp.Country_to == 'Greece')]\n",
    "cp = cp.sort_values(by = ['startingBin'])\n",
    "result = cp.groupby(['grouppedDays']).agg({'avg(Full_Price)': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(num = 1, figsize = (10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.array(result.index)\n",
    "y = np.array(result['avg(Full_Price)'])\n",
    "p1 = np.polyfit(x, y, 2)\n",
    "y1 = np.polyval(p1, x)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, y1)\n",
    "plt.subplot(1,2,2)\n",
    "x = x[x < 60]\n",
    "y = y[0:x.shape[0]]\n",
    "p2 = np.polyfit(x, y, 1)\n",
    "y2 = np.polyval(p2, x)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y[0:len(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy, że mam już df ze wczytanymi danymi z dwóch tygodni oraz dodana jest kolumna days oraz new_days (z tymi przedzialami dni). Generuję z pliku, który jest już ustrukturyzowany.\n",
    "Bo inaczej to dostanę dużo jakiegoś syfu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muszę to częściowo zwinąć do takiego "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Scrap_date' , 'Scrap_time', 'Country_from', 'Country_to', 'new_days']).agg(\n",
    "F.mean(df.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Te new_days to trzeba zrobić jakoś tak, że\n",
    "1 - 5\n",
    "6 - 10\n",
    "11 - 15\n",
    "16 - 20\n",
    "21 - 25\n",
    "26 - 30\n",
    "31 - 35\n",
    "36 - 40\n",
    "41 - 45\n",
    "46 - 50\n",
    "51 < X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Grupuję po ['Scrap_date' , 'Scrap_time', 'Country_from', 'Country_to', 'new_days']\n",
    "2. Agreguję średnią oraz wariancję \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\n",
    "'1-5',\n",
    "'6-10',\n",
    "'11-15',\n",
    "'16-20',\n",
    "'21-25',\n",
    "'26-30',\n",
    "'31-35',\n",
    "'36-40',\n",
    "'41-45',\n",
    "'46-50',\n",
    "'51-1000']\n",
    "means = [40, 37, 36, 30, 29, 27, 20, 19, 19, 16, 15]\n",
    "sd = np.flip(np.random.exponential(scale = 2, size = 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.random.exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'levels': levels, 'means': means, 'sd': sd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['level_down'] = [int(lev.split('-')[0]) for lev in df['levels']]\n",
    "df['level_up'] = [int(lev.split('-')[1]) for lev in df['levels']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albo nawet zrobic tak, ze zrobic bin dla kazdego dnia i po prostu, jak jakiegos nie ma to nie uwzledniac go w dopasowaniu krzywej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.groupby(['Scrap_time', 'Country_from', 'Country_to', 'Days']).agg(\n",
    "    F.mean(dt.Full_Price),\n",
    "    F.stddev(dt.Full_Price)\n",
    ")\n",
    "\n",
    "dt.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [12 , 13, 14, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,11, num = 11)\n",
    "y = df['means']\n",
    "p = np.polyfit(x,y, 2)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, np.polyval(p, x))\n",
    "y2 = np.polyval(p, x2) + np.random.randn(len(x2))\n",
    "plt.plot(x2,y2,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(1,11, num = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1,11, num = 11), df['means'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,11, num = 11)\n",
    "y = df['means']\n",
    "p = np.polyfit(x,y, 2)\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, np.polyval(p, x))\n",
    "x2 = [12 , 13, 14, 15]\n",
    "sds = [s for s in df['sd'] if ]\n",
    "y2 = np.polyval(p, x2) + np.random.randn(np.polyval(p, x2))\n",
    "plt.plot(x2,y2,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [12 , 13, 14, 15]\n",
    "y2 = np.polyval(p, x2)\n",
    "plt.plot(x2,y2,'o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
